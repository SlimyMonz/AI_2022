{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"19ldCWlJ7REW"},"source":["# Reinforcement Learning\n","\n","This is the 5th assignment for CAP 4630 and we will train an AI-based explorer to play a game by reinforcement learing. As domestrated below, in this game, the treasure (denoted by T) is on the right-most and the explorer (denoted by o) will learn to get the treasure by moving left and right. The explorer will be rewarded when it gets the treasure.  After serveral epoches, the explorer will learn how to get the treasure faster and finally it will go to the treasure by moving to right directly. \\\n","\n","Episode 1, Step1: o----T   \\\n","... \\\n","Episode 1, Step6: ---o-T   \\\n","... \\\n","Episode 1, Step10: -o---T \\\n","... \\\n","Episode 1, Step15: ----oT (finished) \\\n","\n","You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 Points)** \\\n","\n","**Task Overview:**\n","- Train the explorer getting the treasure quickly through Q-learning method"]},{"cell_type":"markdown","metadata":{"id":"p0Cmvbnj7REa"},"source":["## 1 Achieve Q-learning method ##\n","### 1.1 Model Preparation**(5 Points)**\n","\n","Import useful packages and prepare hyperpaprameters for Q-learning methods. \n","\n","**Tasks:**\n","1. Import numpy and rename it to np.\n","2. Import pandas and rename it to pd.\n","3. Import the library \"time\"\n","4. Set the parameter as suggested\n","\n","**Hints:**\n","1. For your first trial, you may set as it is\n","2. You may explore other possibilities here when you complete the whole homework"]},{"cell_type":"code","metadata":{"id":"j4i1-N7o7REb","executionInfo":{"status":"ok","timestamp":1669503561275,"user_tz":300,"elapsed":98,"user":{"displayName":"Harry Hocker","userId":"12836174997390486614"}}},"source":["#import packages here\n","import numpy as np\n","import pandas as pd\n","import time\n","\n","N_STATES = 6   \n","ACTIONS = ['left', 'right']\n","EPSILON = 0.9\n","ALPHA = 0.1\n","GAMMA = 0.9\n","MAX_EPISODES = 13\n","FRESH_TIME = 0.3 "],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y6x0keZ07REc"},"source":["### 1.2 Q table**(10 Points)**\n","\n","Q table is a [states * actions] matrix, which stores Q-value of taking one action in that specific state. For example, the following Q table means in state s3, it is more likely to choose a1 because it's Q-value is 5.31 which is higher than Q-value 2.33 for a0 in s3(refer to Lecture slides 16, page 35).\n","![](https://drive.google.com/uc?export=view&id=1WGh7NYyYw6ccrxbDVdfbJmb_IhBfUyFf)\n","\n","**Tasks:**\n","1. define the build_q_table function\n","2. **Print Out** defined Q-table\n","\n","**Hints:**\n","1. Using pd.DataFrame to define the Q-table.(https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n","2. Initialize the Q-table with all zeros."]},{"cell_type":"code","metadata":{"id":"f3VlakdE7REc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669503561514,"user_tz":300,"elapsed":131,"user":{"displayName":"Harry Hocker","userId":"12836174997390486614"}},"outputId":"3c5c15d5-8823-4dd4-f62c-96907ab2494b"},"source":["#define the function here\n","def build_q_table(NS, A):\n","    QT = pd.DataFrame(np.zeros((NS, len(A))), columns=A,)\n","    return QT\n","\n","\n","q_table = build_q_table(N_STATES, ACTIONS)\n","print(q_table)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["   left  right\n","0   0.0    0.0\n","1   0.0    0.0\n","2   0.0    0.0\n","3   0.0    0.0\n","4   0.0    0.0\n","5   0.0    0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"SvGAtNQm7REe"},"source":["### 1.3 Define action**(15 Points)**\n","\n","In this section, we are going to define how an actor picks the actions. We introduce ε-greedy (In lecture slide 16, page 35). In the initial exploring stage, the explorer knows little about the environment. Therefore, it is better to explore randomly instead of greedy. ε-greedy is the value to control the degree of greedy. It can be changed with time lapsing. In this homework, we set it as fixed value EPSILON = 0.9. You can change it to explore the final effect.\n","\n","**Tasks:**\n","1. define the choose_action function\n","2. **Print Out** sample action.\n","\n","**Hints:**\n","1. You need to define two patterns: 1) non-greedy (i.e., random); 2) greedy.\n","2. Non-greedy should occupy (1-ε) senario while greedy should occupy ε senario. In this case, it means Non-greedy occupys 10% senario while greedy occupys 90% senario. (you could implement it by comparing a random number ranging from 0 to 1 with ε)\n","3. In the non-greedy pattern, the actor should choose the actions randomly.\n","4. In the greedy pattern, the actor should choose the higher Q-value action.\n","5. Don't forget the initial state which means all Q-value are zero and actor cannot choose greedily. You can treat it as non-greedy pattern."]},{"cell_type":"code","metadata":{"id":"Qps8xVq87REe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669503561515,"user_tz":300,"elapsed":7,"user":{"displayName":"Harry Hocker","userId":"12836174997390486614"}},"outputId":"7f13bae6-d842-42dc-82c6-123a002d0db8"},"source":["#define the function here\n","# Given state and Q-table, choose action\n","def choose_action(S, qT):\n","    sA = qT.iloc[S, :]\n","    if (np.random.uniform() > EPSILON) or ((sA == 0).all()):\n","        aN = np.random.choice(ACTIONS)\n","    else:\n","        aN = sA.idxmax()\n","    return aN\n","\n","sample_action = choose_action(0, q_table)\n","print(sample_action)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["left\n"]}]},{"cell_type":"markdown","metadata":{"id":"1y0sb9Wr84rV"},"source":["### 1.4 Interact with the environment**(30 Points)**\n","\n","In this section, we need to give a feedback for our previous action, which means getting reward (R) for next state (S_next) based on current state (S_current) and action (A). In this problem, we get reward R=1 if we move to the treasure T spot, otherwise, we get R=0.\n","\n","**Tasks:**\n","1. define get_env_feedback function\n","**Hints:**\n","1. This function contains two parameters S_current and A(ction), and return S_next and R(eward).\n","2. You need to consider two different senarios: 1) A = right; 2) A = left.\n","3. In the above two senarios, you need to consider the boundary, next state and rewards.\n","4. The update_env function is given to show changes for different steps in different episodes.\n","5. The validation for S_current and Action is shown below.\n","\n","- S_current=0, sample_action = 'right', sample_feedback=(1,0)\n","- S_current=3, sample_action = 'right', sample_feedback=(4,0)\n","- S_current=4, sample_action = 'right', sample_feedback=('terminal', 1)\n","- S_current=0, sample_action = 'left', sample_feedback=(0,0)\n","- S_current=3, sample_action = 'left', sample_feedback=(2,0)\n","- S_current=4, sample_action = 'left', sample_feedback=(3, 0)"]},{"cell_type":"code","metadata":{"id":"3Xb5Xi8U7REf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669503561515,"user_tz":300,"elapsed":6,"user":{"displayName":"Harry Hocker","userId":"12836174997390486614"}},"outputId":"fb87f97f-bf30-4376-eae6-eb2b6af815e8"},"source":["#define the function here\n","def get_env_feedback(S, A):\n","    if A == 'right':\n","        if S == N_STATES - 2:\n","            S_ = 'terminal'\n","            R = 1\n","        else:\n","            S_ = S + 1\n","            R = 0\n","    else:\n","        R = 0\n","        if S == 0:\n","            S_ = S \n","        else:\n","            S_ = S - 1\n","    return S_, R\n","\n","sample_action = 'left'\n","S_current = 4\n","sample_feedback = get_env_feedback(S_current, sample_action)\n","print(sample_feedback)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 0)\n"]}]},{"cell_type":"code","metadata":{"id":"R5X__swLIJd3","executionInfo":{"status":"ok","timestamp":1669503561516,"user_tz":300,"elapsed":5,"user":{"displayName":"Harry Hocker","userId":"12836174997390486614"}}},"source":["def update_env(S, E, SC):\n","    eList = ['-']*(N_STATES-1) + ['T']\n","    if S == 'terminal':\n","        I = 'Episode %s: total_steps = %s' % (E+1, SC)\n","        print('\\r{}'.format(I), end='')\n","        time.sleep(2)\n","        print('\\r                                ', end='')\n","    else:\n","        eList[S] = 'o'\n","        I = ''.join(eList)\n","        print('\\r{}'.format(I), end='')\n","        time.sleep(FRESH_TIME)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSESWvv5ANHl"},"source":["### 1.5 Start Q-learning with defined functions**(40 Points)**\n","\n","In this section, we are going to utilize all the functions defined above to do q-learning based on the optimal policy.\n","![](https://drive.google.com/uc?export=view&id=10ra6mLlBHlhGNTYWwdGANoa6lC1K_7at)\n","\n","**Tasks**:\n","1. define reinforce_learning function\n","\n","**Hints**:\n","1. You should write this function with loops to keep updating q-table until you get to the reward spot.\n","2. We have two loops, one is for different episodes and another one is for steps\n","3. Whenever we take a step to the reward spot, we should end the loop and start another episode.\n","4. Here is one possible example.\n","\n","![](https://drive.google.com/uc?export=view&id=1oo-gk710XVXbbeI7AI0uZInrnKtqGqn7)"]},{"cell_type":"code","metadata":{"id":"4t3feBszAZUx","executionInfo":{"status":"ok","timestamp":1669503561517,"user_tz":300,"elapsed":6,"user":{"displayName":"Harry Hocker","userId":"12836174997390486614"}}},"source":["#define the function here\n","def reinforce_learning():\n","    QT = build_q_table(N_STATES, ACTIONS)\n","    for E in range(MAX_EPISODES):\n","        SC = 0\n","        S = 0\n","        T = False\n","        update_env(S, E, SC)\n","        while not T:\n","            A = choose_action(S, QT)\n","            S_, R = get_env_feedback(S, A)\n","            pred = QT.loc[S, A]\n","            if S_ != 'terminal':\n","                target = R + GAMMA * QT.iloc[S_, :].max()\n","            else:\n","                target = R\n","                T = True\n","\n","            QT.loc[S, A] += ALPHA * (target - pred)\n","            S = S_\n","\n","            update_env(S, E, SC+1)\n","            SC += 1\n","    return QT"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSRgnyFdAdfx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669503615024,"user_tz":300,"elapsed":53512,"user":{"displayName":"Harry Hocker","userId":"12836174997390486614"}},"outputId":"4a9ae750-9b56-41c2-e20e-057c95de2ea6"},"source":["#main function to run \n","if __name__ == \"__main__\":\n","    q_table = reinforce_learning()\n","    print('\\r\\nQ-table:\\n')\n","    print(q_table)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["                                \n","Q-table:\n","\n","      left     right\n","0  0.00000  0.004513\n","1  0.00000  0.025589\n","2  0.00000  0.109208\n","3  0.00027  0.340790\n","4  0.00000  0.745813\n","5  0.00000  0.000000\n"]}]}]}